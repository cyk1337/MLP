{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-05 20:26:32,739 : INFO : collecting all words and their counts\n",
      "2018-02-05 20:26:32,741 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-05 20:26:32,742 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-02-05 20:26:32,743 : INFO : Loading a fresh vocabulary\n",
      "2018-02-05 20:26:32,744 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-02-05 20:26:32,745 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-02-05 20:26:32,746 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-02-05 20:26:32,747 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-02-05 20:26:32,748 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-02-05 20:26:32,751 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-02-05 20:26:32,752 : INFO : resetting layer weights\n",
      "2018-02-05 20:26:32,754 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-05 20:26:32,758 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-05 20:26:32,759 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-05 20:26:32,761 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-05 20:26:32,762 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-05 20:26:32,764 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-05 20:26:32,765 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-05 20:26:32,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-05 20:26:32,767 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-05 20:26:32,770 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-05 20:26:32,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-05 20:26:32,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-05 20:26:32,772 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 337 effective words/s\n",
      "2018-02-05 20:26:32,774 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-05 20:26:32,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-05 20:26:32,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-05 20:26:32,777 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-05 20:26:32,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-05 20:26:32,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-05 20:26:32,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-05 20:26:32,783 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-02-05 20:26:32,784 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 35 effective words/s\n",
      "2018-02-05 20:26:32,785 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['first', 'sentence', 'second'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LabeledSentence'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8cc2893a6e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LabeledSentence'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from gensim import utils  \n",
    "from gensim.models.doc2vec import LabeledSentence  \n",
    "from gensim.models import Doc2Vec  \n",
    "from random import shuffle  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "import sklearn.metrics as metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):  \n",
    "    def __init__(self, sources):  \n",
    "        self.sources = sources  \n",
    "  \n",
    "        flipped = {}  \n",
    "  \n",
    "        # make sure that keys are unique  \n",
    "        for key, value in sources.items():  \n",
    "            if value not in flipped:  \n",
    "                flipped[value] = [key]  \n",
    "            else:  \n",
    "                raise Exception('Non-unique prefix encountered')  \n",
    "  \n",
    "    def __iter__(self):  \n",
    "        for source, prefix in self.sources.items():  \n",
    "            with utils.smart_open(source) as fin:  \n",
    "                for item_no, line in enumerate(fin):  \n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])  \n",
    "  \n",
    "    def to_array(self):  \n",
    "        self.sentences = []  \n",
    "        for source, prefix in self.sources.items():  \n",
    "            with utils.smart_open(source) as fin:  \n",
    "                for item_no, line in enumerate(fin):  \n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))  \n",
    "        return self.sentences  \n",
    "  \n",
    "    def sentences_perm(self):  \n",
    "        shuffle(self.sentences)  \n",
    "        return self.sentences  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
