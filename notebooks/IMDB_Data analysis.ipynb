{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "---\n",
    "\n",
    "- Separation of data into training and test sets.\n",
    "- Loading and cleaning the data to remove punctuation and numbers.\n",
    "- Defining a vocabulary of preferred words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import __init__\n",
    "from config.setting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Separation of data into training and test sets.\n",
    "- Training data\n",
    "   - pos: 12,500\n",
    "   - neg: 12,500\n",
    "- Test data\n",
    "   - pos: 12,500\n",
    "   - neg: 12,500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Loading and cleaning the data to remove punctuation and numbers.\n",
    "1. Split tokens on white space.\n",
    "2. Remove all punctuation from words.\n",
    "3. Lowercase.\n",
    "4. Remove Html tag (data crawling fault..)\n",
    "\n",
    "> TBC\n",
    "5. Remove  stop words.\n",
    "6. Remove all words that have a length <= 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['always', 'feel', 'strange', 'guilty', 'saying', 'Im', 'fairly', 'welleducated', 'nonteenager', 'actually', 'sort', 'like', 'Olsen', 'twins', 'respect', 'movies', 'make', 'even', 'though', 'Ive', 'never', 'really', 'target', 'audience', 'When', 'Rome', 'traditional', 'MaryKate', 'Ashley', 'movie', 'complete', 'foreign', 'travel', 'accents', 'motorbikes', 'adult', 'romance', 'storyline', 'fashion', 'orientation', 'even', 'gag', 'reel', 'credits', 'enjoyed', 'When', 'Rome', 'Olsen', 'twin', 'movies', 'never', 'pretend', 'anything', 'theyre', 'time', 'premiere', 'video', 'never', 'claim', 'next', 'Citizen', 'Kane', 'even', 'An', 'Affair', 'Remember', 'My', 'point', 'people', 'watch', 'movie', 'expect', 'anything', 'another', 'Olsen', 'twin', 'movie', 'disappointedbr', 'br', 'That', 'said', 'ARE', 'fans', 'Olsen', 'twins', 'really', 'enjoy', 'For', 'us', 'whove', 'watched', 'since', 'first', 'episodes', 'Full', 'House', 'really', 'great', 'see', 'growing', 'mature', 'roles', 'This', 'movie', 'provides', 'important', 'historical', 'geographical', 'information', 'like', 'many', 'movies', 'remember', 'Downing', 'Street', 'Winning', 'London', 'visit', 'Louvre', 'Passport', 'Paris', 'well', 'providing', 'good', 'clean', 'fun', 'enjoyed', 'whole', 'familybr', 'br', 'As', 'long', 'still', 'feel', 'like', 'Im', 'soapbox', 'long', 'make', 'relevant', 'movie', 'let', 'take', 'moment', 'challenge', 'reject', 'Olsen', 'twins', 'order', 'fan', 'Olsen', 'twins', 'dont', 'preteen', 'valley', 'girl', 'California', 'In', 'fact', 'thats', 'really', 'target', 'audience', 'If', 'MKA', 'fashion', 'line', 'clothes', 'accessories', 'would', 'run', 'Gap', 'store', 'like', 'WalMart', 'When', 'Rome', 'feature', 'high', 'fashion', 'globetrotting', 'two', 'girls', 'valley', 'Cali', 'isnt', 'really', 'ABOUT', 'inspiring', 'young', 'girls', 'initiative', 'let', 'take', 'places', 'If', 'means', 'setting', 'movie', 'glamorous', 'foreign', 'city', 'cute', 'guys', 'motorbikes', 'Thats', 'called', 'marketingyou', 'take', 'idea', 'sell', 'making', 'appealing', 'At', 'least', 'theyre', 'sending', 'good', 'message', 'even', 'means', 'seem', 'little', 'superficial', 'br', 'br', 'Basically', 'dont', 'knock', 'film', 'youve', 'seen', 'dont', 'knock', 'youve', 'tried', 'understand', 'Olsen', 'twins', 'encourage', 'young', 'girls', 'creative', 'intuitive', 'driven', 'young', 'women', 'This', 'movie', 'think', 'like', 'others', 'Kids', 'enjoy', 'Parents', 'If', 'like', 'Olsen', 'twins', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "# example of single txt process\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " \n",
    "# load the document\n",
    "filename = train_pos+'/12499_7.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining a vocabulary of preferred words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137159\n",
      "[('movie', 41802), ('film', 37460), ('one', 25153), ('like', 19561), ('good', 14508), ('even', 12325), ('would', 12124), ('time', 11783), ('really', 11639), ('story', 11431), ('see', 11176), ('much', 9548), ('get', 9200), ('well', 9192), ('people', 8922), ('also', 8894), ('bad', 8891), ('great', 8847), ('first', 8687), ('dont', 8304), ('made', 7984), ('movies', 7776), ('films', 7725), ('make', 7719), ('could', 7707), ('way', 7682), ('characters', 7283), ('think', 7215), ('watch', 6708), ('two', 6581), ('many', 6579), ('seen', 6525), ('character', 6511), ('never', 6402), ('little', 6379), ('acting', 6254), ('plot', 6242), ('best', 6234), ('love', 6206), ('know', 6040), ('life', 5984), ('show', 5958), ('ever', 5805), ('better', 5547), ('still', 5485), ('end', 5352), ('say', 5327), ('man', 5192), ('scene', 5169), ('scenes', 5059)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "    \n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # clean html tag\n",
    "    doc = cleanhtml(doc)\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # TODO: stemming?\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "    \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    " \n",
    "\n",
    "# load all docs\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "#         if is_trian and filename.endswith('.txt'):\n",
    "#             continue\n",
    "#         if not is_trian and not filename.end('.txt'):\n",
    "#             continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(train_neg, vocab, True)\n",
    "process_docs(train_pos, vocab, True)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 21.804 words (std: 264.064)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEkFw0GSVDAcJEkFw0GSVPh/G3Vwx6aGZAkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b37f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot word frequency\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "word_freq = list(vocab.values())\n",
    "print(\"Mean %.3f words (std: %.3f)\" % (np.mean(word_freq), np.std(word_freq) ))\n",
    "_ = plt.boxplot(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55904\n"
     ]
    }
   ],
   "source": [
    "# filter words whose occurance less than 2\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens)) # before 137159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary_.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabulary_.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # clean html tag\n",
    "    doc = cleanhtml(doc)\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # TODO: stemming?\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table).lower() for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train=True):\n",
    "    \"\"\"is_train: flag, do not use here\"\"\"\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs(train_pos, vocab)\n",
    "negative_docs = process_docs(train_neg, vocab)\n",
    "train_docs = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len: 9213, min len: 27\n",
      "Mean 842.108 words (std: 650.490)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUAaAJBXKAJCkQv0fnfUU48tzb+kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be61940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot word frequency\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "review_len = [len(text) for text in train_docs]\n",
    "print(\"max len: %.0f, min len: %.0f\" % (max(review_len), min(review_len)))\n",
    "print(\"Mean %.3f words (std: %.3f)\" % (np.mean(review_len), np.std(review_len) ))\n",
    "_ = plt.boxplot(review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# length of encoded docs\n",
    "# need to ensure that all documents have the same length.\n",
    "# ===========\n",
    "_ = set([len(i) for i in encoded_docs])\n",
    "# ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403\n"
     ]
    }
   ],
   "source": [
    "# length of sequence\n",
    "# ===========\n",
    "dim_ = list(set([len(i) for i in Xtrain]))[0]\n",
    "print(dim_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 277,
   "position": {
    "height": "40px",
    "left": "825px",
    "right": "20px",
    "top": "138px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
